{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Numerical computation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Machine Learning & Data Processing\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Numerical computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning & Data Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Deep Learning (Neural Networks)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check TensorFlow and Keras versions\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Keras Version:\", keras.__version__)\n",
    "\n",
    "# Check installed versions of other libraries\n",
    "import sklearn\n",
    "print(\"Scikit-Learn Version:\", sklearn.__version__)\n",
    "import seaborn as sns\n",
    "print(\"Seaborn Version:\", sns.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_tokens(text):\n",
    "    filtered_text=re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "    filtered_text=filtered_text.split()\n",
    "    filtered_text=[token.lower() for token in filtered_text]\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(data_text,min_frequency=5):\n",
    "    review_tokens=[get_processed_tokens(review) for review in data_text]\n",
    "    token_list=[token for review in review_tokens for token in review]\n",
    "    token_freq_dict={token:token_list.count(token) for token in token_list}\n",
    "    most_freq_tokens=[tokens for tokens in  token_freq_dict if token_freq_dict[tokens]>=min_frequency]\n",
    "    idx=range(len(most_freq_tokens))\n",
    "\n",
    "    token_idx=dict(zip(most_freq_tokens,idx))\n",
    "    return token_idx,len(token_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(data):\n",
    "    tokens_per_review=[len(txt.split()) for txt in data]\n",
    "    return max(tokens_per_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(data_text,token_idx,max_tokens):\n",
    "    review_tokens=[get_processed_token(review) for review in data_text]\n",
    "    review_token_idx=map(lambda review:[token_idx[k] for k in review if k in token_idx.keys()],\n",
    "                         review_tokens)\n",
    "    padded_sequences=padded_sequences(review_token_idx,maxlen=max_tokens)\n",
    "    return np.array(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(num_tokens,max_tokens):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim=num_tokens,output_dim=EMBEDDING_SIZE,input_length=max_tokens,name='layer_embedding'))\n",
    "    model.add(GRU(units=16,name=\"gru_1\",return_sequences=True))\n",
    "    model.add(GRU(units=8,name=\"gru_2\",return_sequences=True))\n",
    "    model.add(GRU(units=4,name=\"gru_3\"))\n",
    "    model.add(Dense(1,activation='sigmoid',name=\"dense_1\"))\n",
    "    optimizer=Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,inout_sequence,y_train):\n",
    "    model.fit(input_sequence,y_train,batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and preprocess the data\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming the format is: text\\tlabel\n",
    "            text, label = line.strip().split('\\t')\n",
    "            data.append({'text': text, 'label': int(label)})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the text data\"\"\"\n",
    "    # Convert labels to binary (assuming 0 is negative, 1 is positive)\n",
    "    df['label'] = df['label'].map({0: 0, 1: 1})\n",
    "    return df\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train the sentiment analysis model\"\"\"\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    \n",
    "    # Transform the training data\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    return model, vectorizer\n",
    "\n",
    "def evaluate_model(model, vectorizer, X_test, y_test):\n",
    "    \"\"\"Evaluate the model and create confusion matrix\"\"\"\n",
    "    # Transform test data\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    train_df = load_data('Dataset/training.txt')\n",
    "    train_df = preprocess_data(train_df)\n",
    "    \n",
    "    # Split data into features and labels\n",
    "    X = train_df['text']\n",
    "    y = train_df['label']\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model, vectorizer = train_model(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set:\")\n",
    "    evaluate_model(model, vectorizer, X_val, y_val)\n",
    "    \n",
    "    # Load and evaluate on test data\n",
    "    print(\"\\nLoading and evaluating on test data...\")\n",
    "    test_df = load_data('Dataset/testdata.txt')\n",
    "    test_df = preprocess_data(test_df)\n",
    "    evaluate_model(model, vectorizer, test_df['text'], test_df['label'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()'\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_unique_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
